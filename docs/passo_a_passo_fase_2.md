# Fase 2 ‚Äî Processing Service (Silver)

## 1. Objetivo da Fase 2

Implementar o **microservi√ßo de processamento** respons√°vel por:

*   Ler os dados brutos (JSON) da camada **Bronze** (MinIO/S3).
*   **Limpar e Normalizar** o texto (NLTK: remo√ß√£o de stopwords, caracteres especiais, min√∫sculas).
*   Gerar **Embeddings Vetoriais** do resumo dos artigos (BERT/Transformers).
*   Salvar os dados processados (Estruturados + Vetor) na camada **Silver**.

> [!NOTE]
> **Arquitetura (Worker vs API):**
> Embora estruturado como um microservi√ßo independente (DB pr√≥prio, deploy isolado), este componente comporta-se primariamente como um **Worker orientado a Jobs**. A API (FastAPI) sugerida abaixo serve como interface de controle/gatilho para orquestradores (como Airflow ou cron jobs), e n√£o para atender requisi√ß√µes de lat√™ncia ultra-baixa de usu√°rios finais.

---

## 2. Pr√©-requisitos

### 2.1 Software necess√°rio

*   Python **3.12+**
*   Docker & Docker Compose
*   Poetry
*   Hardware sugerido: M√≠nimo 4GB RAM (devido ao modelo BERT)

---

## 3. Cria√ß√£o do Projeto

```bash
cd processing_service
git init
```

### 3.1 Ignorando arquivos (.gitignore)

Utilize o mesmo padr√£o da Fase 1, adicionando exclus√µes para modelos pesados se baixados localmente:
```gitignore
__pycache__/
*.py[cod]
.venv/
.env
.pytest_cache/
.coverage
htmlcov/
dist/
# Modelos
models/
cache/

### 3.2 Ignorando contexto Docker (.dockerignore)
Para evitar enviar arquivos desnecess√°rios (e pesados, como `venv` ou `cache`) para o build do Docker, crie o arquivo `.dockerignore`:

```text
.git
.gitignore
__pycache__/
*.pyc
*.pyo
*.pyd

.env
.env.*

.venv/
venv/

data/
models/
cache/
artifacts/

node_modules/
dist/
build/

*.ipynb
*.csv
*.parquet
*.pdf
```
```

---

## 4. Estrutura de Diret√≥rios (Domain-Driven Design)

Adotaremos uma estrutura baseada em DDD para isolar a complexidade do ML:

```text
processing_service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/                <-- (Opcional se for API, ou worker.py se for Job)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py       <-- Configura√ß√µes (Env vars)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py      <-- Inje√ß√£o de Depend√™ncia
‚îÇ   ‚îú‚îÄ‚îÄ domain/             <-- (N√∫cleo: Regras de Neg√≥cio Puras)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py       <-- Entidade Article (agora com vetor)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ports.py        <-- Interfaces (Repository, Cleaner, Embedder)
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/     <-- (Adaptadores: Implementa√ß√µes)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ s3_repository.py      <-- Leitura/Escrita no MinIO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ regex_cleaner.py      <-- Implementa√ß√£o Leve (Sem NLTK)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bert_embedder.py      <-- Implementa√ß√£o de ML (Transformers)
‚îÇ   ‚îú‚îÄ‚îÄ services/           <-- (Casos de Uso)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processor_service.py  <-- Orquestrador (L√™ -> Limpa -> Embed -> Salva)
|   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_cleaner.py
‚îÇ   ‚îú‚îÄ‚îÄ test_repository.py
‚îÇ   ‚îú‚îÄ‚îÄ test_service.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

---

## 5. Configura√ß√£o do Projeto (pyproject.toml)

```toml
[tool.poetry]
name = "processing-service"
version = "0.1.0"
description = "Servi√ßo de Processamento - Camada Silver (NLP + Embeddings)"
authors = ["Seu Nome"]
packages = [{include = "app"}]

[tool.poetry.dependencies]
python = "^3.12"
fastapi = "^0.115"
uvicorn = "^0.30"
pydantic = "^2.9"
pydantic-settings = "^2.6"
boto3 = "^1.34"
# ML & NLP
torch = { version = "^2.2", source = "pytorch-cpu" }
transformers = "^4.39"
numpy = "^1.26"

[[tool.poetry.source]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
priority = "explicit"
# NLTK removido para evitar downloads em rede na produ√ß√£o

[tool.poetry.group.dev.dependencies]
pytest = "^8.2"
pytest-asyncio = "^1.3.0"
black = "^24.4"
ruff = "^0.4"
bump2version = "^1.0.1"
moto = "^5.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

---

### 5.1 Versionamento Automatizado (`.bumpversion.cfg`)

Arquivo de configura√ß√£o para versionamento autom√°tico do `pyproject.toml`.

```ini
[bumpversion]
current_version = 0.1.0
commit = True
tag = True

[bumpversion:file:pyproject.toml]
search = version = "{current_version}"
replace = version = "{new_version}"
```

### 5.2 Governan√ßa e Seguran√ßa

Para garantir a integridade do projeto e evitar commits inseguros, foram adicionados dois mecanismos de controle.

#### 1. `AGENTS.MD` (Regras da IA)
Arquivo na raiz que define as "Leis do Projeto" para agentes de IA:
- Proibido alterar `.ignore` para ler secrets.
- Respeito estrito √† estrutura do Monorepo.
- Testes devem usar Mocks.

#### 2. Git Hooks (`.pre-commit-config.yaml`)

**Seguran√ßa Refor√ßada: Git Hooks Instalados! üõ°Ô∏è**

Atendendo ao seu pedido (e ao bom senso), configurei o pre-commit no projeto.

O que foi feito:
1. Criei o arquivo de configura√ß√£o `.pre-commit-config.yaml` na raiz.
2. Instalei a ferramenta via pip.
3. Registrei o hook no Git (`.git/hooks/pre-commit`).

**Como isso te protege agora?**
Toda vez que voc√™ rodar `git commit`, o hook vai checar automaticamente (em segundos):

*   ‚úÖ **Secrets:** Se voc√™ tentar commitar uma chave AWS ou senha (como eu tentei editar o ignore), o commit ser√° BLOQUEADO.
*   ‚úÖ **Arquivos Gigantes:** Impede commitar arquivos > 5MB (ex: modelos `.bin` do BERT ou arquivos `.csv` gigantes que deveriam estar no S3).
*   ‚úÖ **Sintaxe:** Verifica se quebrou algum YAML ou TOML.

**Test Drive:**
Ao fazer seu pr√≥ximo commit, voc√™ ver√° algo como:
```text
detect-secrets................................................Passed
check-added-large-files.......................................Passed
```

**Conte√∫do de `.pre-commit-config.yaml` (Raiz):**
```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
        args: ['--maxkb=5000'] # 5MB limit (prevent models being committed)

  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.5.0
    hooks:
      - id: detect-secrets
        args: ['--baseline', '.secrets.baseline']
        exclude: package.lock.json|poetry.lock
```

---

## 6. Vari√°veis de Ambiente (.env)

```env
# Infraestrutura
USE_S3=True
S3_ENDPOINT=http://localhost:9000
S3_BUCKET_BRONZE=arxiv-bronze
S3_BUCKET_SILVER=arxiv-silver
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
AWS_REGION=us-east-1

# Modelo
MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

```

---

## 6.1 Automa√ß√£o (Makefile)

O `Makefile` simplifica comandos comuns. Crie o arquivo na raiz do `processing_service`:

```makefile
.PHONY: install test format lint run clean bump-minor

install:
	poetry install

test:
	poetry run pytest tests/ -v

format:
	poetry run ruff check --fix .
	poetry run black .

lint:
	poetry run ruff check .
	poetry run black --check .

run:
	# Roda apenas o banco/infra se necess√°rio, ou o servi√ßo completo
	docker compose up --build -d

clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete

bump-minor:
	# Sobe vers√£o 0.1.0 -> 0.2.0
	poetry run bump2version minor --verbose

```

### 6.2 Preparando o Ambiente (Instala√ß√£o)

Agora que os arquivos de configura√ß√£o est√£o prontos, **execute os comandos abaixo** para gerar o arquivo de trava (lockfile) e instalar as depend√™ncias:

1.  **Gerar Lockfile (Importante para Docker):**
    Este passo gera o `poetry.lock` resolvendo as depend√™ncias (CPU-only) que ser√£o copiadas para o container.
    ```bash
    cd processing_service
    poetry lock
    ```

2.  **Instalar Depend√™ncias:**
    ```bash
    make install
    # ou "poetry install"
    ```

3.  **Instalar Hooks de Seguran√ßa (Local):**
    Para ativar a valida√ß√£o autom√°tica (pre-commit) no seu ambiente de desenvolvimento:
    ```bash
    poetry add --group dev pre-commit detect-secrets
    poetry run pre-commit install

    # Gerar baseline de segredos (necess√°rio na raiz do workspace)
    cd ..
    poetry run detect-secrets scan > .secrets.baseline
    cd processing_service
    ```
---

## 7. Estrat√©gia de Idempot√™ncia (Controle de Estado)
Para evitar processamento duplicado (custo computacional de BERT √© alto), implementaremos uma checagem simples:

1.  **Identidade:** O nome do arquivo na Bronze (`{id}.json`) ser√° mantido na Silver.
2.  **Verifica√ß√£o:** Antes de processar, o servi√ßo verificar√° se o arquivo `{id}.json` j√° existe no bucket Silver.
3.  **Listagem Inteligente:** O m√©todo `list_unprocessed_files` deve, idealmente, listar apenas a diferen√ßa entre Bronze e Silver (set difference), ou o worker verifica um a um (mais simples para MVP).

> [!WARNING]
> **Race Conditions (Concorr√™ncia):**
> Em um cen√°rio com m√∫ltiplos workers rodando em paralelo, pode ocorrer uma *race condition* onde dois workers verificam a inexist√™ncia do arquivo simultaneamente e ambos tentam process√°-lo. Para este MVP, assumiremos risco aceit√°vel. Solu√ß√µes futuras incluem: *S3 Conditional Writes* ou *Distributed Locks* (Redis/DynamoDB).

---

### 7.4 Resili√™ncia e Observabilidade (Melhorias Recomendadas)

Para um ambiente produtivo robusto na AWS, recomenda-se adicionar:

1.  **Retries Autom√°ticos (Tenacity):**
    O acesso ao S3 pode falhar intermitentemente. Use a lib `tenacity` para retentativas exponenciais.
    ```python
    from tenacity import retry, stop_after_attempt, wait_exponential

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def get_raw_article_with_retry(self, key):
        return await self.repo.get_raw_article(key)
    ```

2.  **Logs Estruturados (Structlog):**
    Logs em texto plano s√£o dif√≠ceis de consultar no CloudWatch/Datadog. Use `structlog` para output JSON.
    ```python
    # Exemplo de log estruturado
    logger.info("processing_article", article_id="123", outcome="success", duration_ms=150)
    # Output: {"event": "processing_article", "article_id": "123", "outcome": "success", "duration_ms": 150}
    ```

---

### 7.5 Escalabilidade e Performance (Limita√ß√µes vs Produ√ß√£o)

Para grandes volumes (>10k artigos), a abordagem atual possui gargalos que devem ser tratados no futuro:

1.  **Listagem de Arquivos (Set Difference):**
    A checagem item-a-item (`exists_in_silver`) √© O(N) e lenta com muitos arquivos.
    *   **Solu√ß√£o:** Listar todos keys do Bronze, todos keys do Silver, e computar a diferen√ßa em mem√≥ria (Set Difference) para processar apenas o delta.

2.  **Hardware (CPU vs GPU):**
    BERT √© intensivo em computa√ß√£o matricial.
    *   **CPU:** ~0.5 a 2 segundos por artigo. Lote de 1000 = ~30 minutos.
    *   **GPU (CUDA):** ~10 a 50x mais r√°pido. Essencial para reprocessamentos massivos.
    *   **Mitiga√ß√£o MVP:** Usar modelos "Mini" ou "Tiny" (ex: `all-MiniLM-L6-v2`) e manter `batch_size` pequeno.

---

### 7.6 Seguran√ßa e Conformidade

1.  **Gest√£o de Segredos (Credentials):**
    *   **Local/Dev:** O uso de `.env` com `minioadmin` √© aceit√°vel **apenas** para desenvolvimento local isolado.
    *   **Produ√ß√£o:** NUNCA commite o `.env`. Utilize **AWS Secrets Manager**, **HashiCorp Vault** ou inje√ß√£o de vari√°veis de ambiente seguras via CI/CD (GitHub Secrets) e IAM Roles para ECS/Lambda.

2.  **Privacidade e PII (LGPD/GDPR):**
    *   **ArXiv:** Dados s√£o p√∫blicos, risco reduzido.
    *   **Dados Gerais:** Se processar dados privados, embeddings podem "memorizar" PII (CPF, E-mail).
    *   **Mitiga√ß√£o:** Antes de gerar o embedding, aplicar saneamento de PII (ex: biblioteca *Microsoft Presidio* ou Regex espec√≠fico para anonimiza√ß√£o).

### 7.7 Qualidade de C√≥digo e Documenta√ß√£o

1.  **README.md Rico:**
    *   Deve conter comandos expl√≠citos para: **Setup** (`poetry install`), **Testes** (`pytest`), e **Execu√ß√£o** (`docker compose up`).
    *   **M√©tricas de Performance:** Documentar o tempo m√©dio por artigo (CPU vs GPU) para alinhar expectativas de processamento em lote.

2.  **Type Hints Consistentes:**
    *   Evitar tipos gen√©ricos como `dict` ou `Any`. Preferir Pydantic Models ou `TypedDict`.
    *   Exemplo: Em `get_raw_article`, retornar um Schema definido em vez de `dict` solto.
    *   Utilizar `mypy` no CI para garantir consist√™ncia.

---

## 8. C√≥digo-Fonte Principal

### 8.0 Core (Configura√ß√£o e Logs)

#### `app/core/config.py`
Utiliza Pydantic Settings para valida√ß√£o robusta das vari√°veis de ambiente.

```python
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    USE_S3: bool = True
    S3_ENDPOINT: str = "http://minio:9000"
    S3_BUCKET_BRONZE: str = "arxiv-bronze"
    S3_BUCKET_SILVER: str = "arxiv-silver"
    AWS_ACCESS_KEY_ID: str = "minioadmin"
    AWS_SECRET_ACCESS_KEY: str = "minioadmin"
    AWS_REGION: str = "us-east-1"
    MODEL_NAME: str = "sentence-transformers/all-MiniLM-L6-v2"

    model_config = SettingsConfigDict(env_file=".env")


settings = Settings()

```

#### `app/core/logger.py`
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("processing-service")
```

---

### 8.1 Domain (Camada Pura)

#### `app/domain/models.py`
A grande mudan√ßa aqui √© a adi√ß√£o do campo para o embedding vetorial e texto limpo.

```python
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime


class ArticleAttributes(BaseModel):
    id: str
    title: str
    summary: str
    cleaned_summary: Optional[str] = None
    embedding: Optional[List[float]] = None
    categories: List[str]
    published: datetime
    # Outros campos herdados do raw...

    # Validator opcional para garantir dimens√£o correta do modelo (ex: MiniLM = 384)
    # @field_validator('embedding')
    # def check_embedding_dim(cls, v):
    #     if v and len(v) != 384: raise ValueError("Invalid embedding dimension")
    #     return v
```

#### `app/domain/ports.py` (Interfaces)
Definimos contratos para que o Service n√£o dependa de bibliotecas concretas (NLTK, PyTorch).

```python
from typing import Protocol, List, Any
from app.domain.models import ArticleAttributes


class RepositoryProtocol(Protocol):
    async def get_raw_article(self, file_key: str) -> dict: ...
    async def save_processed_article(self, article: ArticleAttributes) -> None: ...
    async def list_unprocessed_files(self) -> List[str]: ...
    async def exists_in_silver(self, article_id: str) -> bool: ...


class CleanerProtocol(Protocol):
    def clean_text(self, text: str) -> str: ...


class EmbedderProtocol(Protocol):
    def generate_embedding(self, text: str) -> List[float]: ...
```

---

### 8.2 Infrastructure (Adaptadores)

#### `app/infrastructure/regex_cleaner.py`
> [!IMPORTANT]
> **Decis√£o de Arquitetura (NLTK vs Regex):**
> Optei por uma **abordagem nativa com Regex** em vez do NLTK para esta implementa√ß√£o. Embora o NLTK seja poderoso, ele introduz riscos operacionais significativos (downloads externos no startup, alto consumo de mem√≥ria) desnecess√°rios para uma limpeza simples de stopwords. A implementa√ß√£o via Regex √© **stateless**, n√£o requer downloads de rede e elimina um ponto √∫nico de falha na infraestrutura AWS.

```python
import re
from app.domain.ports import CleanerProtocol


class RegexCleaner(CleanerProtocol):
    def __init__(self):
        # Stopwords m√≠nimas hardcoded para evitar depend√™ncia externa
        # Stopwords m√≠nimas hardcoded para evitar depend√™ncia externa
        self.stop_words = {
            # Artigos e conjun√ß√µes
            "a",
            "an",
            "the",
            "and",
            "or",
            "but",
            # Preposi√ß√µes
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "as",
            "by",
            "from",
            "into",
            "about",
            "between",
            "during",
            "via",
            # Verbos auxiliares comuns
            "is",
            "are",
            "was",
            "were",
            "can",
            "could",
            "may",
            "might",
            "should",
            "would",
            # Pronomes
            "it",
            "we",
            "our",
            "this",
            "these",
            "those",
            "each",
            "both",
            "which",
            # Palavras gen√©ricas em artigos cient√≠ficos
            "that",
            "such",
            "other",
            "another",
            "also",
            "using",
            "used",
            "based",
            "than",
            "however",
            "although",
            "while",
            "thus",
            "hence",
        }

    def clean_text(self, text: str) -> str:
        if not text:
            return ""

        # 1. Lowercase
        text = text.lower()

        # 2. Remove caracteres especiais (mant√©m letras e espa√ßos)
        # OBS: Remove acentos e n√∫meros. Para suporte multi-idioma, ajustar regex.
        text = re.sub(r"[^a-zA-Z\s]", "", text)

        # 3. Tokeniza√ß√£o simples (split por espa√ßo)
        tokens = text.split()

        # 4. Remove stopwords
        filtered_tokens = [w for w in tokens if w not in self.stop_words and len(w) > 2]

        return " ".join(filtered_tokens)

```

#### `app/infrastructure/bert_embedder.py`
```python
from transformers import AutoTokenizer, AutoModel
import torch
from app.domain.ports import EmbedderProtocol
from app.core.config import settings


class BERTEmbedder(EmbedderProtocol):
    def __init__(self):
        # Carrega modelo pr√©-treinado (leve)
        self.tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_NAME)
        self.model = AutoModel.from_pretrained(settings.MODEL_NAME)

    def generate_embedding(self, text: str) -> list[float]:
        # WARN: Em multi-thread (asyncio.gather), o modelo compartilhado pode sofrer race conditions.
        # Se escalar, usar thread-local storage ou locks.
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True, max_length=512
        )
        with torch.no_grad():
            outputs = self.model(**inputs)

        # Mean Pooling para obter um vetor √∫nico por senten√ßa
        embeddings = outputs.last_hidden_state
        attention_mask = inputs["attention_mask"]

        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
        sum_embeddings = torch.sum(embeddings * mask_expanded, 1)
        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)

        mean_pooled = sum_embeddings / sum_mask
        # Normaliza√ß√£o (opcional, bom para similaridade de cosseno)
        return mean_pooled[0].tolist()


```



### `app/infrastructure/s3_repository.py`
```python
import json
from typing import List
import boto3
from app.domain.ports import RepositoryProtocol
from app.domain.models import ArticleAttributes
from app.core.config import settings


class S3Repository(RepositoryProtocol):
    def __init__(self):
        self.s3 = boto3.client(
            "s3",
            endpoint_url=settings.S3_ENDPOINT,
            aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
            aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
            region_name=settings.AWS_REGION,
        )
        self._ensure_buckets_exist()

    def _ensure_buckets_exist(self):
        for bucket in [settings.S3_BUCKET_BRONZE, settings.S3_BUCKET_SILVER]:
            try:
                self.s3.head_bucket(Bucket=bucket)
            except self.s3.exceptions.ClientError:
                # Bucket likely missing, try to create
                try:
                    self.s3.create_bucket(Bucket=bucket)
                    print(f"Bucket {bucket} created.")
                except Exception as e:
                    print(f"Failed to create bucket {bucket}: {e}")

    async def list_unprocessed_files(self) -> List[str]:
        # 1. Listar tudo no Bronze (Pagina√ß√£o s3.list_objects_v2 omitida para brevidade)
        bronze_objs = self.s3.list_objects_v2(Bucket=settings.S3_BUCKET_BRONZE).get(
            "Contents", []
        )
        bronze_keys = {obj["Key"] for obj in bronze_objs}

        # 2. Listar tudo no Silver
        silver_objs = self.s3.list_objects_v2(Bucket=settings.S3_BUCKET_SILVER).get(
            "Contents", []
        )
        silver_ids = {obj["Key"].replace(".json", "") for obj in silver_objs}

        # 3. Set Difference (O(1) lookup)
        # Se bronze tem "folder/123.json", extra√≠mos "123" para comparar.
        unprocessed = []
        for key in bronze_keys:
            article_id = key.split("/")[-1].replace(".json", "")
            if article_id not in silver_ids:
                unprocessed.append(key)

        return unprocessed

    async def get_raw_article(self, file_key: str) -> dict:
        response = self.s3.get_object(Bucket=settings.S3_BUCKET_BRONZE, Key=file_key)
        return json.loads(response["Body"].read())

    async def save_processed_article(self, article: ArticleAttributes) -> None:
        key = f"{article.id}.json"
        self.s3.put_object(
            Bucket=settings.S3_BUCKET_SILVER,
            Key=key,
            Body=article.model_dump_json(indent=2),
            ContentType="application/json",
        )

    async def exists_in_silver(self, article_id: str) -> bool:
        # Esta implementa√ß√£o √© O(1) se o bucket for pequeno, mas O(N) para listar tudo.
        # A list_unprocessed_files j√° faz uma checagem mais eficiente para o batch.
        # Para checagem individual, pode-se tentar get_object e capturar ClientError.
        try:
            self.s3.head_object(
                Bucket=settings.S3_BUCKET_SILVER, Key=f"{article_id}.json"
            )
            return True
        except self.s3.exceptions.ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            raise  # Re-raise other errors
```

### 10.1 Service (Tests com Mocks)

Este teste garante que o fluxo (orquestra√ß√£o) est√° correto, usando mocks para isolar reposit√≥rio e modelos.

```python
# tests/test_service.py
import pytest
from unittest.mock import AsyncMock, Mock
from app.services.processor_service import ProcessingService
from app.domain.models import ArticleAttributes


@pytest.mark.asyncio
async def test_processing_service_flow():
    # Mocks
    mock_repo = AsyncMock()
    mock_cleaner = Mock()
    mock_embedder = Mock()

    # Configura√ß√£o
    mock_repo.exists_in_silver.return_value = False
    mock_repo.get_raw_article.return_value = {
        "article_data": {
            "id": "123",
            "title": "Test",
            "summary": "Raw summary",
            "categories": ["cs.AI"],
            "published": "2024-01-01",
        }
    }
    mock_cleaner.clean_text.return_value = "cleaned summary"
    mock_embedder.generate_embedding.return_value = [0.1, 0.2]

    service = ProcessingService(mock_repo, mock_cleaner, mock_embedder)

    # Execu√ß√£o
    await service.process_one_file("123.json")

    # Verifica√ß√£o
    # 1. Garante que processou
    mock_cleaner.clean_text.assert_called_with("Raw summary")
    mock_embedder.generate_embedding.assert_called_with("cleaned summary")

    # 2. Garante que salvou
    mock_repo.save_processed_article.assert_called_once()
    saved_article = mock_repo.save_processed_article.call_args[0][0]
    assert saved_article.cleaned_summary == "cleaned summary"
    assert saved_article.embedding == [0.1, 0.2]
```

### 10.2 Cleaner (Unit Test)

```python
# tests/test_cleaner.py
from app.infrastructure.regex_cleaner import RegexCleaner


def test_cleaner_removes_stopwords_and_special_chars():
    cleaner = RegexCleaner()
    raw_text = "The AI is great! It's 100% efficient."
    # Esperado: remove 'the', 'is', 'it', '!', '100%', ' efficient' -> 'great efficient'
    # Nota: nossa regex remove numeros e palavras < 3 chars
    result = cleaner.clean_text(raw_text)
    assert "great" in result
    assert "efficient" in result
    assert "100" not in result
    assert "!" not in result


def test_cleaner_edge_cases():
    cleaner = RegexCleaner()
    # Caso vazio
    assert cleaner.clean_text("") == ""
    # Caso apenas stopwords
    assert cleaner.clean_text("the and of") == ""
    # Caso caracteres menores que 3
    assert cleaner.clean_text("ab cd") == ""
```

### 10.3 Repository (Moto - S3 Mock)
Simula o S3 AWS localmente.

```python
# tests/test_repository.py
import pytest
import boto3
from moto import mock_aws
from app.infrastructure.s3_repository import S3Repository
from app.core.config import settings



### 8.4 API (Interface)

#### `app/api/routes.py`
```python
from fastapi import APIRouter, Depends
from typing import Annotated
from app.infrastructure.s3_repository import S3Repository
from app.infrastructure.regex_cleaner import RegexCleaner
from app.infrastructure.bert_embedder import BERTEmbedder
from app.services.processor_service import ProcessingService

router = APIRouter()


# Dependency Factory
def get_processor_service():
    repo = S3Repository()
    cleaner = RegexCleaner()
    embedder = BERTEmbedder()
    return ProcessingService(repo, cleaner, embedder)


@router.post("/process_batch")
async def process_batch(
    service: Annotated[ProcessingService, Depends(get_processor_service)],
    limit: int = 10,
):
    files = (
        await service.repo.list_unprocessed_files()
    )  # Acesso direto ao repo injetado
    count = 0
    for f in files[:limit]:
        await service.process_one_file(f)
        count += 1

    return {"status": "ok", "processed": count}
```

### 8.5 Main (Entrypoint)
#### `app/main.py`
```python
from fastapi import FastAPI
from app.api.routes import router

app = FastAPI(title="Processing Service (Phase 2)")

app.include_router(router)
```

---

## 9. Testes Automatizados


@pytest.fixture
def s3_mock():
    with mock_aws():
        # Patch S3_ENDPOINT to None so S3Repository uses standard AWS URLs (intercepted by moto)
        with patch.object(settings, "S3_ENDPOINT", None):
            # Setup do ambiente fake
            s3 = boto3.client("s3", region_name="us-east-1")
            s3.create_bucket(Bucket=settings.S3_BUCKET_BRONZE)
            s3.create_bucket(Bucket=settings.S3_BUCKET_SILVER)
            yield s3


@pytest.mark.asyncio
async def test_repo_exists_silver(s3_mock):
    repo = S3Repository()
    # Verifica inexist√™ncia
    assert await repo.exists_in_silver("fake_id") == False

    # Cria arquivo fake na silver e verifica
    s3_mock.put_object(Bucket=settings.S3_BUCKET_SILVER, Key="exist.json", Body="{}")
    assert await repo.exists_in_silver("exist") == True
```
---
### 10.4 Executando os Testes

Ap√≥s criar os arquivos acima, execute a su√≠te de testes para validar a implementa√ß√£o:

```bash
make test
# Output esperado: 100% passing
```
---
### 10.5 Rodando a Aplica√ß√£o

Com os testes aprovados, suba o ambiente:

```bash
make run
# O servi√ßo estar√° rodando em http://localhost:8001
# Docs interativa: http://localhost:8001/docs
```
---

## 11. Dockerfile

O Dockerfile precisar√° instalar depend√™ncias de sistema para o compilador do Torch (se necess√°rio) e fazer cache dos downloads do modelo para n√£o baixar a cada build/run.

```dockerfile
# Use uma imagem slim
FROM python:3.12-slim

WORKDIR /app

# Instala Poetry
RUN pip install poetry

# Copia arquivos de config
COPY pyproject.toml poetry.lock ./

# Instala depend√™ncias
RUN poetry config virtualenvs.create false \
    && poetry install --no-interaction --no-ansi --no-root

# Models cache e outras otimiza√ß√µes podem ser adicionadas aqui

# Copia c√≥digo
COPY . .

# CMD padr√£o para API
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001"]

# OBS: Para rodar em modo Worker (batch), sobrescreva o CMD no docker-compose ou runtime:
# CMD ["python", "app/services/processor_service.py", "--limit", "100"]
```

## 11.1 Docker Compose (docker-compose.yml)

Crie o arquivo `docker-compose.yml` na raiz:

```yaml
services:
  processing_service:
    build: .
    container_name: processing_service
    ports:
      - "8001:8001"
    env_file:
      - path: .env
        required: false
    environment:
      - S3_ENDPOINT=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_REGION=us-east-1
    depends_on:
      - minio
    volumes:
      - ./app:/app/app # Hot-reload
    command: uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
    networks:
      - arxiv-shared

  minio:
    image: minio/minio
    container_name: arxiv_minio_silver
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin # pragma: allowlist secret
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      arxiv-shared:
        aliases:
          - arxiv-minio

volumes:
  minio_data:
    external: true
    name: ingestion_service_minio_data

networks:
  arxiv-shared:
    external: true

```

## 12. Pr√≥ximos Passos (Fase 3)

Com os dados na **Silver** (limpos e vetorizados), a pr√≥xima fase (**Frontend**) poder√°:
1.  Carregar esses dados.
2.  Permitir busca sem√¢ntica (comparando vetor da query com vetores dos artigos).
3.  Exibir resultados de forma rica no Streamlit.

---

## 13. Versionamento e Deploy (Git Flow)

Seguindo sua diretriz, a sequ√™ncia de commits para esta fase deve ser:

1.  **Primeiro Commit (Estrutura Inicial):**
    Adiciona todo o c√≥digo fonte da Fase 2 (Processing Service).
    ```bash
    git add .
    git commit -m "feat: implement initial structure for processing service (phase 2)"
    ```

2.  **Segundo Commit (Release v0.2.0):**
    Utiliza o `bump2version` para subir a vers√£o de **0.1.0** para **0.2.0**.
    ```bash
    # Na raiz do workspace (onde est√° o .bumpversion.cfg principal, se houver, ou dentro do servi√ßo)
    # Assumindo que o bumpversion gerencia a vers√£o global ou do servi√ßo:
    poetry run bump2version minor --verbose
    # Output esperado: v0.1.0 -> v0.2.0
    # Isso criar√° automaticamente o commit e a tag "v0.2.0"
    ```

3.  **Push:**
    ```bash
    git push origin main --tags
    ```
